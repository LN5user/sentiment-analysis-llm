{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b52d12",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Consumer Reviews of Amazon Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa2afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c540a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 300\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from Emoticon import EMOTICONS_EMO\n",
    "from sklearn.utils import shuffle\n",
    "import torch \n",
    "from torch import nn\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc6285b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMOTICONS_EMO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626201f2",
   "metadata": {},
   "source": [
    "### Data Description "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a05027",
   "metadata": {},
   "source": [
    "For this project I used the data provided by Kaggle [Consumer Reviews of Amazon Products](\n",
    "https://www.kaggle.com/datasets/datafiniti/consumer-reviews-of-amazon-products). The dataset includes rating, review text, category, timestamp etc. for each product.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a02cf7",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment(row):\n",
    "    \"\"\" Convert rating to setiment\n",
    "    Input:\n",
    "        pandas Series row\n",
    "    Output \n",
    "        pandas Series row\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    if row >= 4:\n",
    "        sentiment = 1\n",
    "    else:\n",
    "        sentiment = 0\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predictions(row):\n",
    "    \"\"\" Convert predicted sentiment to nummeric output\n",
    "    Input:\n",
    "        pandas Series row\n",
    "    Output \n",
    "        pandas Series row\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    if row[0][\"label\"]=='POSITIVE':\n",
    "        sentiment = 1\n",
    "    elif row[0][\"label\"]=='NEGATIVE':\n",
    "        sentiment = 0\n",
    "    else:\n",
    "        sentiment = np.nan\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fd4b5",
   "metadata": {},
   "source": [
    "## 1. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888cb597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed108ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categories'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty reviews and rating\n",
    "df = df[(df['reviews.text'].isnull()==False)&(df['reviews.rating'].isnull()==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x=\"reviews.rating\")\n",
    "plt.xlabel('rating');\n",
    "plt.title(\"Distributions of Rating Score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025e358",
   "metadata": {},
   "source": [
    "Turning to the graphic, it shows the scores from minimal 1 and maximum 5. \n",
    "Interesting is that the data has a lot of better score (4-5) and less negative (1-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f651186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rating to sentiment classes (positive and negative)\n",
    "df.loc[:,'label'] = df['reviews.rating'].apply(create_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['reviews.text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'reviews.text': \"text\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d77495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates  \n",
    "df.drop_duplicates(subset=['text'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c45c25",
   "metadata": {},
   "source": [
    "The text in comments is quite clean without html tags and with very few links. So for that reason, I will only convert emojis to text. In addition, for data Data augmentation (Section 3) I will use the technique of back translation. Therefore, I will not lowercase the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d958db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoticons_to_words(row):\n",
    "    \"\"\" Convert emojis in comments into text\n",
    "    Input:\n",
    "        pandas Series row\n",
    "    Output \n",
    "        pandas Series row\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    for i, j in EMOTICONS_EMO.items():\n",
    "        row = row.replace(i, j)\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(convert_emoticons_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_class_distribution(data):\n",
    "    \"\"\"\n",
    "    Visualisation of the proportions \n",
    "    for each class\n",
    "    Input:\n",
    "        data: pandas dataframe\n",
    "    Output:\n",
    "        None\n",
    "    \n",
    "    \"\"\"\n",
    "    colors = ['royalblue', 'pink']\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pie(data['label'].value_counts(), labels=[\"positive\",\"negative\"], autopct='%1.1f%%', colors=colors)\n",
    "    plt.title(\"Sentiment Class Distribution\");\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d9e7f",
   "metadata": {},
   "source": [
    "The graph below makes it clear that the class of negative reviews is disproportionally underrepresented. This challenge will be handled in the section \"3. Data augmentation\" and \"5. Data balancing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_class_distribution(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804fb691",
   "metadata": {},
   "source": [
    "## 2. Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cce8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into training(60%), validation(20%) and test set (20%)\n",
    "train, validate, test = \\\n",
    "              np.split(df.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(df)), int(.8*len(df))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cdf443",
   "metadata": {},
   "source": [
    "## Using Basic \"sentiment-roberta-large-english\" Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a95501",
   "metadata": {},
   "source": [
    "As the baseline I use [SiEBERT - English-Language Sentiment Classification](https://huggingface.co/siebert/sentiment-roberta-large-english) model which enables to predict positive and negative sentiment. The model is a fine-tuned checkpoint of RoBERTa-large (Liu et al. 2019) and outperformed DistilBERT SST-2\n",
    "on average by  15 p.p. (78.1 vs 93.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6eedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_classifier = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\",truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data_test, classifier):\n",
    "    \"\"\"\n",
    "    Create new column with predictions\n",
    "    Input:\n",
    "        data_test: DataFrame\n",
    "    Output:\n",
    "        data_test: DataFrame (with predictions)\n",
    "    \"\"\"\n",
    "    data_test['roberta_sentiment'] = data_test['text'].apply(lambda x : classifier(x))\n",
    "    data_test.loc[:,'predicted'] =data_test[\"roberta_sentiment\"].apply(convert_predictions)\n",
    "    return data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e3fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_basis = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_basis_prediction = get_predictions(test_basis, roberta_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d881a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_basis_prediction.label, test_basis_prediction.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8749909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any unpredicted comments\n",
    "test_basis_prediction[test_basis_prediction.predicted.isnull()==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b7953",
   "metadata": {},
   "source": [
    "## Fine-tuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20df42",
   "metadata": {},
   "source": [
    "## 3. Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f22c9",
   "metadata": {},
   "source": [
    "For data augmentation I will use the technique [“Back Translation”](https://towardsdatascience.com/nlp-data-augmentation-using-transformers-89a44a993bab).  First of all, I translate the English text into German and then back to English. The major reasons for this technique are\n",
    "1.\tThere are enough good performing models for English \n",
    "2.\tDue to the fact, that the task is sentiment analysis (the polarity of the sentences). I don’t want to add noise to the dataset with random replacement, random insertion or text generation etc.\n",
    "3. With Back Translation we will create new samples for the minority class (oversampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#English to German\n",
    "translation_en_to_de = pipeline(\"translation_en_to_de\", model='t5-base')\n",
    "\n",
    "#Germal to English\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert2bert_L-24_wmt_de_en\", pad_token=\"<pad>\", eos_token=\"</s>\", bos_token=\"<s>\")\n",
    "model_de_to_en = AutoModelForSeq2SeqLM.from_pretrained(\"google/bert2bert_L-24_wmt_de_en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1dfae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translation(input_text):\n",
    "    \"\"\" Translate the text from English to German\n",
    "        and then from German to English\n",
    "    Input:\n",
    "        pandas Series row\n",
    "    Output \n",
    "        pandas Series row\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    review_en_to_de = translation_en_to_de(input_text)\n",
    "    text_en_to_de = review_en_to_de[0]['translation_text']\n",
    "    input_ids = tokenizer(text_en_to_de, return_tensors=\"pt\", add_special_tokens=False,max_length=512,truncation=True).input_ids\n",
    "    output_ids = model_de_to_en.generate(input_ids)[0]\n",
    "    augmented_review = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return augmented_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_samples(data):\n",
    "    \"\"\"\n",
    "    Create new samples for training\n",
    "    Input:\n",
    "        data: pandas data frame\n",
    "    Output:\n",
    "        data: pandas data frame with additional sampels\n",
    "    \n",
    "    \"\"\"\n",
    "    count_labels= data[\"label\"].value_counts()\n",
    "    n = count_labels[1]-count_labels[0]\n",
    "    # oversampling for negative class\n",
    "    data_temp = data[data.label==0].sample(n=n, replace=True, random_state=1)\n",
    "    data_temp.loc[:,'samples'] =data_temp[\"text\"].apply(back_translation)\n",
    "    data_temp = data_temp.drop('text', axis=1)\n",
    "    data_temp.rename(columns={'samples': \"text\"},inplace=True)\n",
    "    data_sampled = pd.concat([data_temp, data], ignore_index=True)\n",
    "    data_sampled = shuffle(data_sampled, random_state=0)\n",
    "    return data_sampled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3d99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = create_data_samples(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1ddad",
   "metadata": {},
   "source": [
    "Looking at the graph below you can see, that now the training dataset is balanced. This dataset will be used for the second approach (Data Balancing - Oversampling)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_class_distribution(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5add11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5112306",
   "metadata": {},
   "source": [
    "## 4. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12053d2d",
   "metadata": {},
   "source": [
    "One of the parameter of tokenizer is max_length. So, before I use default number, let us check, what is the max number of the reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaffcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78885250",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.from_pandas(train).remove_columns(['__index_level_0__'])\n",
    "validate = Dataset.from_pandas(validate).remove_columns(['__index_level_0__'])\n",
    "test = Dataset.from_pandas(test).remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cdfcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_len(data):\n",
    "    \"\"\"\n",
    "    Calculate the length of each sentence\n",
    "    Input:\n",
    "        data: Dataset\n",
    "    Output:\n",
    "        tokens_len: list with length of each sentence\n",
    "    \n",
    "    \"\"\"\n",
    "    tokens_len = []\n",
    "    for review in data[\"text\"]:\n",
    "        tokens = tokenizer.encode(review)\n",
    "        tokens_len.append(len(tokens))\n",
    "    return tokens_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf956b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of tokens\n",
    "np.array(get_review_len(train)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65827263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min number of tokens\n",
    "np.array(get_review_len(train)).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(get_review_len(train),bins=20)\n",
    "plt.xlabel('tokent count');\n",
    "plt.title(\"Distributions of Tokens per Sentence\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901cf89",
   "metadata": {},
   "source": [
    "The graph above makes it clear that the most of the review are below 180 tokens. For that reason, I will set max_lenght to 180 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(data):\n",
    "    \"\"\"\n",
    "    Input data: Dataset\n",
    "    Output: data: Dataset: with 'input_ids' and 'attention_mask'\n",
    "        \n",
    "    \"\"\"\n",
    "    return tokenizer(data['text'], padding=\"max_length\", max_length=180,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc4a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = train.map(preprocess_function, batched=True)\n",
    "tokenized_val= validate.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9239d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to speed up the training, the training samples will be converted to Pytorch tensors  \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d01c5",
   "metadata": {},
   "source": [
    "For the evaluation of the model performance, I will use on the one hand, accuracy, on the other hand f1 score (as we have an imbalanced dataset). In comparison to accuracy, the f1-score pays more attention to false negative and false positive predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4daee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Defines the evaluation metrics for fine-tuned model\n",
    "    \n",
    "    \"\"\"\n",
    "    load_accuracy = load_metric(\"accuracy\")\n",
    "    load_f1 = load_metric(\"f1\")\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    \n",
    "    print(\"eval_pred\", type(eval_pred))\n",
    "    print(\"accuracy\", type(accuracy),accuracy)\n",
    "    print(\"f1\", type(f1),f1)\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508e772",
   "metadata": {},
   "source": [
    "## 5. Data balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f02cb4",
   "metadata": {},
   "source": [
    "Since the dataset is imbalanced, I will test two approaches\n",
    "1. I will integrate the weighted loss into the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer). Usually, the data points from each class are treated the same. That means,  that the model will learn more for the class which is overrepresented in the dataset. In order to overcome this issue, the weights for minority classes will be increased. This behavior will increase the loss as well and so force the model to pay more attention to these samples. \n",
    "2. Random oversampling the minority of class through Back Translation (see function create_data_samples). In this case, the \"CLASS_WEIGHTS\" will be balanced and so have no substational impact on CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5117de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_WEIGHTS=class_weight.compute_class_weight('balanced',classes=np.unique(train[\"label\"]),y=train[\"label\"])\n",
    "CLASS_WEIGHTS=torch.tensor(CLASS_WEIGHTS,dtype=torch.float,device=\"mps\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf77e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # compute custom loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS,reduction='mean')\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ca8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "   output_dir=\"finetuning-sentiment-roberta-large-english\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=2,\n",
    "   optim = \"adamw_torch\",\n",
    "   save_strategy=\"epoch\",\n",
    "   use_mps_device=\"mps\",\n",
    "   warmup_steps = 500,\n",
    "   weight_decay = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db738cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_val,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99078df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"sentiment-roberta-large-english-fine-tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model_fine_tuned =  pipeline(\"sentiment-analysis\",model=\"sentiment-roberta-large-english-fine-tuned\",truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fine_tuned_prediction = get_predictions(test_basis, sentiment_model_fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b98a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_fine_tuned_prediction.label, test_fine_tuned_prediction.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any unpredicted comments\n",
    "test_fine_tuned_prediction[test_fine_tuned_prediction.predicted.isnull()==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a35e15",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3222d",
   "metadata": {},
   "source": [
    "As shown above the dataset was imbalanced. For this reason, I tested two approaches:\n",
    "1.\tEstimated the weights for imbalanced dataset and forwarded them to the CrossEntropyLoss(). The argument “weight” has impact on the importance of each class \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504e2698",
   "metadata": {},
   "source": [
    "                 precision    recall  f1-score   support\n",
    "\n",
    "           0       0.69      0.53      0.60        45\n",
    "           1       0.98      0.99      0.98       832\n",
    "\n",
    "    accuracy                            0.96       877\n",
    "    macro avg       0.83      0.76      0.79       877\n",
    "    weighted avg    0.96      0.96      0.96       877\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8290b8",
   "metadata": {},
   "source": [
    "2. Random oversampling  the minority of class through Back Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b9d0d",
   "metadata": {},
   "source": [
    "                   precision    recall  f1-score   support\n",
    "\n",
    "           0       0.54      0.49      0.51        45\n",
    "           1       0.97      0.98      0.97       832\n",
    "\n",
    "    accuracy                            0.95       877\n",
    "    macro avg       0.75      0.73      0.74       877\n",
    "    weighted avg    0.95      0.95      0.95       877\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147b5e7",
   "metadata": {},
   "source": [
    "Overall our sentiment analysis shows, that estimation of the weights for imbalanced dataset has more impact on the f-score for negative class and improved the model performance compared to the baseline by 0.03 for negative class and by 0.01 for positive. \n",
    "\n",
    "Furthermore, the technique used was to help us to compare the three models namely: oversampling, weights estimation and baseline. In result, we can observe the decrease of the f1-score for negative examples (oversampling) by 0.09 compared to weights estimation and by 0.06 compared to baseline model.  Noticeably, the model has challenges to generalize on the test set and considerable increased the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05eaeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
